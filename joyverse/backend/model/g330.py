# -*- coding: utf-8 -*-
"""Copy of G3304.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v39x8EOtt2DVjwWWsQ3KHJlGGr2GQMIt
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
from scipy.spatial import distance

# Assume df has columns: x_0, y_0, z_0, ..., x_467, y_467, z_467
# and label

# Load your CSV
df = pd.read_csv('/content/drive/MyDrive/landmarks_dataset.csv')

# Separate landmarks and labels
landmarks = df.drop(columns=['label']).values
labels = df['label'].values

# Reshape: (num_samples, 468 points, 3 coords)
landmarks = landmarks.reshape(-1, 468, 3)

print(f"Landmarks shape: {landmarks.shape}")  # (num_samples, 468, 3)

# Feature Engineering
# ===========================

def compute_features(landmarks_batch):
    features = []

    for face in landmarks_batch:
        single_feature = []

        # --- Important Landmark Indices ---
        # (Based on MediaPipe facemesh landmark map)
        left_eye = 33
        right_eye = 263
        nose_tip = 1
        mouth_left = 61
        mouth_right = 291
        chin = 199
        forehead = 10

        # --- 1. Distances ---
        eye_distance = np.linalg.norm(face[left_eye] - face[right_eye])
        eye_nose_distance = np.linalg.norm(face[nose_tip] - (face[left_eye] + face[right_eye]) / 2)
        mouth_width = np.linalg.norm(face[mouth_left] - face[mouth_right])
        nose_chin_distance = np.linalg.norm(face[nose_tip] - face[chin])
        forehead_chin_distance = np.linalg.norm(face[forehead] - face[chin])

        # --- 2. Ratios ---
        eye_mouth_ratio = eye_distance / mouth_width
        nose_face_ratio = eye_nose_distance / forehead_chin_distance

        # --- 3. Angles ---
        # Angle between eyes and nose
        a = face[left_eye]
        b = face[right_eye]
        c = face[nose_tip]

        ba = a - b
        bc = c - b

        cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))
        angle_eye_nose = np.arccos(np.clip(cosine_angle, -1.0, 1.0))
        angle_eye_nose = np.degrees(angle_eye_nose)

        # --- 4. Mouth Opening ---
        mouth_top = 13
        mouth_bottom = 14
        mouth_opening = np.linalg.norm(face[mouth_top] - face[mouth_bottom])

        # --- 5. More distances (optional expansion)
        left_eye_inner = 133
        right_eye_inner = 362
        inner_eye_distance = np.linalg.norm(face[left_eye_inner] - face[right_eye_inner])

        # --- 6. Face width (ear to ear)
        left_ear = 234
        right_ear = 454
        face_width = np.linalg.norm(face[left_ear] - face[right_ear])

        # --- 7. Ratios again
        eye_to_face_ratio = inner_eye_distance / face_width
        mouth_to_face_ratio = mouth_width / face_width

        # --- Add all features
        single_feature.extend([
            eye_distance,
            eye_nose_distance,
            mouth_width,
            nose_chin_distance,
            forehead_chin_distance,
            eye_mouth_ratio,
            nose_face_ratio,
            angle_eye_nose,
            mouth_opening,
            inner_eye_distance,
            face_width,
            eye_to_face_ratio,
            mouth_to_face_ratio
        ])

        features.append(single_feature)

    return np.array(features)

# Apply feature engineering
X_features = compute_features(landmarks)

print(f"‚úÖ New feature shape: {X_features.shape}")  # Should be (num_samples, ~13)

# Encode labels
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_encoded = le.fit_transform(labels)

# Standardize features
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_features)

# Train-val-test split
from sklearn.model_selection import train_test_split

X_temp, X_test, y_temp, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.125, random_state=42)

print(f"Train shape: {X_train.shape}, Val shape: {X_val.shape}, Test shape: {X_test.shape}")

import torch
import torch.nn as nn

class LandmarkTransformer(nn.Module):
    def __init__(self, input_dim, num_classes, d_model=64, nhead=2, num_layers=2, dropout=0.3):
        super(LandmarkTransformer, self).__init__()
        self.embedding = nn.Linear(input_dim, d_model)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=128,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(d_model, num_classes)

    def forward(self, x):
        x = self.embedding(x)
        x = x.unsqueeze(1)  # [batch_size, seq_len=1, d_model]
        x = self.transformer(x)
        x = x.squeeze(1)
        x = self.dropout(x)
        return self.fc(x)

from torch.utils.data import DataLoader, TensorDataset

# Convert to tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val, dtype=torch.long)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

# Dataset and DataLoader
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = LandmarkTransformer(input_dim=X_features.shape[1], num_classes=len(le.classes_)).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)

num_epochs = 100
best_val_acc = 0.0
patience = 10
patience_counter = 0

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for X_batch, y_batch in train_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)

        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    # Validation
    model.eval()
    val_correct = 0
    val_total = 0
    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch)
            _, predicted = torch.max(outputs, 1)
            val_total += y_batch.size(0)
            val_correct += (predicted == y_batch).sum().item()

    val_acc = val_correct / val_total
    print(f"Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss:.4f} | Val Acc: {val_acc*100:.2f}%")

    # Save best model
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        patience_counter = 0
        torch.save(model.state_dict(), 'best_model.pth')
        print("‚úÖ Best model saved.")
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print("‚èπÔ∏è Early stopping triggered.")
            break

# Load best model
model = LandmarkTransformer(input_dim=13, num_classes=len(le.classes_)).to(device)
model.load_state_dict(torch.load('best_model.pth'))
model.eval()


test_correct = 0
test_total = 0
with torch.no_grad():
    for X_batch, y_batch in test_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        outputs = model(X_batch)
        _, predicted = torch.max(outputs, 1)
        test_total += y_batch.size(0)
        test_correct += (predicted == y_batch).sum().item()

test_acc = test_correct / test_total
print(f"üéØ Test Accuracy: {test_acc * 100:.2f}%")

from google.colab import files
files.download('best_model.pth')

